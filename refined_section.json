[
    {
        "section": "Abstract",
        "start": 587
    },
    {
        "section": "Introduction",
        "start": 2854
    },
    {
        "section": "Background",
        "start": 4778
    },
    {
        "section": "Model Architecture",
        "start": 6610
    },
    {
        "section": "Model Architecture",
        "subsection": "3.1 Encoder and Decoder Stacks",
        "start": 7377
    },
    {
        "section": "Model Architecture",
        "subsection": "3.2 Attention",
        "start": 8725
    },
    {
        "section": "Model Architecture",
        "subsection": "3.2.1 Scaled Dot-Product Attention",
        "start": 9263
    },
    {
        "section": "Model Architecture",
        "subsection": "3.2.2 Multi-Head Attention",
        "start": 10784
    },
    {
        "section": "Model Architecture",
        "subsection": "3.2.3 Applications of Attention in our Model",
        "start": 12198
    },
    {
        "section": "Model Architecture",
        "subsection": "3.3 Position-wise Feed-Forward Networks",
        "start": 13410
    },
    {
        "section": "Model Architecture",
        "subsection": "3.4 Embeddings and Softmax",
        "start": 14066
    },
    {
        "section": "Model Architecture",
        "subsection": "3.5 Positional Encoding",
        "start": 15100
    },
    {
        "section": "Why Self-Attention",
        "start": 16503
    },
    {
        "section": "Training",
        "start": 19694
    },
    {
        "section": "Results",
        "start": 22499
    },
    {
        "section": "Conclusion",
        "start": 28764
    }
]